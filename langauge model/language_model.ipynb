{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CUDA: True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Use CUDA:\", USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid = 200\n",
    "embed_dim = 300\n",
    "lr = 10\n",
    "NUM_EPOCHS = 20\n",
    "bptt_len = 60\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train.txt'\n",
    "dev_file = 'dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tok = spacy.load('en')\n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in lm_tok.tokenizer(x)]\n",
    "\n",
    "TEXT = data.ReversibleField(sequential=True, tokenize=spacy_tok,\n",
    "                            lower=True, include_lengths=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.LanguageModelingDataset(train_file, TEXT, newline_eos=True)\n",
    "dev_dataset = datasets.LanguageModelingDataset(dev_file, TEXT, newline_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = \"glove.840B.300d\"\n",
    "TEXT.build_vocab(train_dataset, dev_dataset, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterators\n",
    "train_iter = data.BPTTIterator(train_dataset, batch_size=batch_size, bptt_len=bptt_len, repeat=False, shuffle=True)\n",
    "dev_iter = data.BPTTIterator(dev_dataset, batch_size=batch_size, bptt_len=bptt_len, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26246"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embedding = nn.Embedding(len(TEXT.vocab), embed_dim)\n",
    "embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "embedding.weight.requires_grad = False\n",
    "embedding = embedding.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, embedding, dropout=0.5):\n",
    "        super(LM, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.encoder = embedding\n",
    "        self.rnn = nn.LSTM(ninp, nhid, batch_first=True)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.embed_drop = nn.Dropout(dropout)\n",
    "        self.output_drop = nn.Dropout(dropout)\n",
    "#         self.embed_drop = LockedDropout(dropout)\n",
    "#         self.output_drop = LockedDropout(dropout)\n",
    "\n",
    "        # # tie weights\n",
    "        # self.decoder.weight = self.encoder.weight\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (batch_size, max_len)\n",
    "        :param hidden: ((1, batch_size, nhid), (1, batch_size, nhid))\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        emb = self.embed_drop(self.encoder(inputs))\n",
    "        if hidden:\n",
    "            outputs, hidden = self.rnn(emb, hidden)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(emb)\n",
    "        outputs = self.output_drop(outputs)\n",
    "        decoded = self.decoder(outputs)\n",
    "        return decoded, outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LM(len(TEXT.vocab), embed_dim, nhid, embedding).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(lm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    losses = []\n",
    "    for batch in train_iter:\n",
    "        x, y = batch.text.transpose(0, 1).contiguous().to(device), \\\n",
    "                   batch.target.transpose(0, 1).contiguous().to(device)\n",
    "        \n",
    "        out, _, _ = lm(x)\n",
    "        \n",
    "        out = out.contiguous().view(-1, len(TEXT.vocab))\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = criterion(out, y).to(device)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # _ = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clipping)\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch():\n",
    "    losses = []\n",
    "    for batch in dev_iter:\n",
    "        x, y = batch.text.transpose(0, 1).contiguous().to(device), \\\n",
    "                   batch.target.transpose(0, 1).contiguous().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out, _, _ = lm(x)\n",
    "        \n",
    "        out = out.contiguous().view(-1, len(TEXT.vocab))\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = criterion(out, y).to(device)\n",
    "        losses.append(loss.item())\n",
    "            \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.0529, dev loss: 4.5787\n",
      "train loss: 4.4433, dev loss: 4.3347\n",
      "train loss: 4.2720, dev loss: 4.2306\n",
      "train loss: 4.1831, dev loss: 4.1583\n",
      "train loss: 4.1194, dev loss: 4.1019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b2f6745bf84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdev_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-716d7f4a51c6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss_train = train_epoch()\n",
    "    loss_dev = eval_epoch()\n",
    "    \n",
    "    print('train loss: %.4f, dev loss: %.4f' % (loss_train, loss_dev))\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    dev_losses.append(loss_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
